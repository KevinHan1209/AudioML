{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f284f69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/miniconda3/envs/audioML/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "\n",
    "from train_streaming_ctc import BiLSTMCTC, CharTokenizerCTC\n",
    "\n",
    "# Prefer soundfile backend to avoid torchcodec requirement when loading audio.\n",
    "try:\n",
    "    torchaudio.set_audio_backend(\"soundfile\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CHECKPOINT_PATH = \"bilstm_ctc_checkpoint.pt\"\n",
    "\n",
    "\n",
    "def build_logmel_extractor(\n",
    "    target_sample_rate: int = 16_000,\n",
    "    n_fft: int = 400,\n",
    "    hop_length: int = 160,\n",
    "    n_mels: int = 80,\n",
    "    fmin: float = 0.0,\n",
    "    fmax: float | None = None,\n",
    "):\n",
    "    mel = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=target_sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        f_min=fmin,\n",
    "        f_max=fmax or target_sample_rate / 2,\n",
    "        power=2.0,\n",
    "    )\n",
    "    to_db = torchaudio.transforms.AmplitudeToDB(stype=\"power\")\n",
    "\n",
    "    def extract(waveform: torch.Tensor, sample_rate: int) -> torch.Tensor:\n",
    "        # Collapse stereo to mono and resample if needed, then return [frames, n_mels].\n",
    "        if waveform.ndim == 2:\n",
    "            waveform = waveform.mean(dim=0)\n",
    "        if waveform.ndim == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        if sample_rate != target_sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sample_rate, target_sample_rate)\n",
    "        logmel = to_db(mel(waveform)).squeeze(0)\n",
    "        return logmel.transpose(0, 1)\n",
    "\n",
    "    return extract\n",
    "\n",
    "\n",
    "def load_audio(path: str):\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(path)\n",
    "        return waveform, sample_rate\n",
    "    except ImportError as e:\n",
    "        if \"torchcodec\" not in str(e).lower():\n",
    "            raise\n",
    "    audio_np, sample_rate = sf.read(path, dtype=\"float32\", always_2d=False)\n",
    "    waveform = torch.from_numpy(audio_np)\n",
    "    if waveform.ndim == 1:\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "    elif waveform.ndim == 2:\n",
    "        waveform = waveform.transpose(0, 1)\n",
    "    return waveform, sample_rate\n",
    "\n",
    "\n",
    "def load_bilstm_ctc(checkpoint_path: str = CHECKPOINT_PATH, device: torch.device = DEVICE):\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    tokenizer = CharTokenizerCTC()\n",
    "    tok_state = ckpt.get(\"tokenizer_state\") or {}\n",
    "    if tok_state:\n",
    "        tokenizer.id2ch = tok_state.get(\"id2ch\", tokenizer.id2ch)\n",
    "        tokenizer.blank_id = tok_state.get(\"blank_id\", tokenizer.blank_id)\n",
    "        tokenizer.ch2id = {ch: i for i, ch in enumerate(tokenizer.id2ch)}\n",
    "        tokenizer.vocab_size = len(tokenizer.id2ch)\n",
    "\n",
    "    cfg = ckpt.get(\"config\") or {}\n",
    "    logmel_extractor = build_logmel_extractor(\n",
    "        target_sample_rate=cfg.get(\"sample_rate\", 16_000),\n",
    "        n_fft=cfg.get(\"n_fft\", 400),\n",
    "        hop_length=cfg.get(\"hop_length\", 160),\n",
    "        n_mels=cfg.get(\"n_mels\", 80),\n",
    "        fmin=cfg.get(\"fmin\", 0.0),\n",
    "        fmax=cfg.get(\"fmax\"),\n",
    "    )\n",
    "\n",
    "    model = BiLSTMCTC(\n",
    "        n_mels=cfg.get(\"n_mels\", 80),\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        hidden=cfg.get(\"hidden\", 256),\n",
    "        num_layers=cfg.get(\"num_layers\", 3),\n",
    "        dropout=cfg.get(\"dropout\", 0.1),\n",
    "    ).to(device)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    return model, tokenizer, logmel_extractor\n",
    "\n",
    "\n",
    "def log_add_exp(a: float, b: float) -> float:\n",
    "    if a == -math.inf:\n",
    "        return b\n",
    "    if b == -math.inf:\n",
    "        return a\n",
    "    if a < b:\n",
    "        a, b = b, a\n",
    "    return a + math.log1p(math.exp(b - a))\n",
    "\n",
    "\n",
    "def greedy_decode(log_probs: torch.Tensor, tokenizer: CharTokenizerCTC) -> str:\n",
    "    best_path = log_probs.argmax(dim=-1)\n",
    "    if best_path.ndim == 2:\n",
    "        best_path = best_path[:, 0]\n",
    "    decoded = []\n",
    "    prev = None\n",
    "    for idx in best_path.tolist():\n",
    "        if idx == tokenizer.blank_id:\n",
    "            prev = None\n",
    "            continue\n",
    "        if idx != prev:\n",
    "            decoded.append(tokenizer.id2ch[idx])\n",
    "        prev = idx\n",
    "    return \"\".join(decoded)\n",
    "\n",
    "\n",
    "def ctc_prefix_beam_search(log_probs: torch.Tensor, tokenizer: CharTokenizerCTC, beam_width: int = 10) -> str:\n",
    "    if log_probs.ndim == 3:\n",
    "        log_probs = log_probs[:, 0, :]\n",
    "    log_probs = log_probs.cpu()\n",
    "    T, V = log_probs.shape\n",
    "    beams: dict[tuple[int, ...], tuple[float, float]] = {(): (0.0, -math.inf)}\n",
    "    for t in range(T):\n",
    "        next_beams: dict[tuple[int, ...], tuple[float, float]] = {}\n",
    "        step_log_probs = log_probs[t]\n",
    "        topk_vals, topk_ids = torch.topk(step_log_probs, k=min(beam_width * 2, V))\n",
    "        topk = list(zip(topk_ids.tolist(), topk_vals.tolist()))\n",
    "        for prefix, (p_b, p_nb) in beams.items():\n",
    "            # Extend with blank\n",
    "            p_blank = step_log_probs[tokenizer.blank_id].item()\n",
    "            nb_p_b, nb_p_nb = next_beams.get(prefix, (-math.inf, -math.inf))\n",
    "            nb_p_b = log_add_exp(nb_p_b, p_b + p_blank)\n",
    "            nb_p_b = log_add_exp(nb_p_b, p_nb + p_blank)\n",
    "            next_beams[prefix] = (nb_p_b, nb_p_nb)\n",
    "\n",
    "            # Extend with non-blank tokens\n",
    "            last = prefix[-1] if prefix else None\n",
    "            for idx, p in topk:\n",
    "                if idx == tokenizer.blank_id:\n",
    "                    continue\n",
    "                new_prefix = prefix + (idx,) if idx != last else prefix\n",
    "                nb_p_b, nb_p_nb = next_beams.get(new_prefix, (-math.inf, -math.inf))\n",
    "                if idx == last:\n",
    "                    nb_p_nb = log_add_exp(nb_p_nb, p_b + p)\n",
    "                else:\n",
    "                    nb_p_nb = log_add_exp(nb_p_nb, p_b + p)\n",
    "                nb_p_nb = log_add_exp(nb_p_nb, p_nb + p)\n",
    "                next_beams[new_prefix] = (nb_p_b, nb_p_nb)\n",
    "\n",
    "        def beam_score(item):\n",
    "            p_b, p_nb = item[1]\n",
    "            return log_add_exp(p_b, p_nb)\n",
    "\n",
    "        beams = dict(sorted(next_beams.items(), key=beam_score, reverse=True)[:beam_width])\n",
    "\n",
    "    best_prefix, (p_b, p_nb) = max(beams.items(), key=lambda kv: log_add_exp(kv[1][0], kv[1][1]))\n",
    "    tokens = []\n",
    "    prev = None\n",
    "    for idx in best_prefix:\n",
    "        if idx == tokenizer.blank_id:\n",
    "            prev = None\n",
    "            continue\n",
    "        if idx != prev:\n",
    "            tokens.append(tokenizer.id2ch[idx])\n",
    "        prev = idx\n",
    "    return \"\".join(tokens)\n",
    "\n",
    "\n",
    "def transcribe_waveform(\n",
    "    waveform: torch.Tensor,\n",
    "    sample_rate: int,\n",
    "    model: BiLSTMCTC,\n",
    "    tokenizer: CharTokenizerCTC,\n",
    "    logmel_extractor,\n",
    "    device: torch.device = DEVICE,\n",
    "    decode: str = \"beam\",\n",
    "    beam_width: int = 10,\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        features = logmel_extractor(waveform, sample_rate).unsqueeze(0).to(device)\n",
    "        log_probs = model(features)\n",
    "        if decode == \"beam\":\n",
    "            transcript = ctc_prefix_beam_search(log_probs, tokenizer, beam_width=beam_width)\n",
    "        else:\n",
    "            transcript = greedy_decode(log_probs.cpu(), tokenizer)\n",
    "    return transcript\n",
    "\n",
    "\n",
    "def transcribe_file(\n",
    "    path: str,\n",
    "    model: BiLSTMCTC,\n",
    "    tokenizer: CharTokenizerCTC,\n",
    "    logmel_extractor,\n",
    "    device: torch.device = DEVICE,\n",
    "    decode: str = \"beam\",\n",
    "    beam_width: int = 10,\n",
    ") -> str:\n",
    "    waveform, sample_rate = load_audio(path)\n",
    "    return transcribe_waveform(waveform, sample_rate, model, tokenizer, logmel_extractor, device, decode, beam_width)\n",
    "\n",
    "\n",
    "model, tokenizer, logmel_extractor = load_bilstm_ctc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c155afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from streaming_librispeech_dataset import StreamingLibriSpeechDataset\n",
    "\n",
    "\n",
    "def _canonical_split(split: str) -> str:\n",
    "    aliases = {\n",
    "        \"validation.clean\": \"validation\",\n",
    "        \"dev-clean\": \"validation\",\n",
    "        \"dev\": \"validation\",\n",
    "        \"test.clean\": \"test\",\n",
    "        \"test-clean\": \"test\",\n",
    "    }\n",
    "    return aliases.get(split, split)\n",
    "\n",
    "\n",
    "def _levenshtein(seq1, seq2):\n",
    "    m, n = len(seq1), len(seq2)\n",
    "    dp = list(range(n + 1))\n",
    "    for i in range(1, m + 1):\n",
    "        prev = dp[0]\n",
    "        dp[0] = i\n",
    "        s1 = seq1[i - 1]\n",
    "        for j in range(1, n + 1):\n",
    "            temp = dp[j]\n",
    "            cost = 0 if s1 == seq2[j - 1] else 1\n",
    "            dp[j] = min(dp[j] + 1, dp[j - 1] + 1, prev + cost)\n",
    "            prev = temp\n",
    "    return dp[-1]\n",
    "\n",
    "\n",
    "def _cer(ref: str, hyp: str) -> float:\n",
    "    if not ref:\n",
    "        return 0.0 if not hyp else 1.0\n",
    "    return _levenshtein(ref, hyp) / len(ref)\n",
    "\n",
    "\n",
    "def _wer(ref: str, hyp: str) -> float:\n",
    "    ref_toks, hyp_toks = ref.split(), hyp.split()\n",
    "    if not ref_toks:\n",
    "        return 0.0 if not hyp_toks else 1.0\n",
    "    return _levenshtein(ref_toks, hyp_toks) / len(ref_toks)\n",
    "\n",
    "\n",
    "def evaluate_librispeech(\n",
    "    split: str = \"validation\",\n",
    "    subset: str = \"clean\",\n",
    "    max_samples: int = 20,\n",
    "    decode: str = \"beam\",\n",
    "    beam_width: int = 10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Stream a few samples from LibriSpeech and report CER/WER.\n",
    "    \"\"\"\n",
    "\n",
    "    split = _canonical_split(split)\n",
    "\n",
    "    dataset = StreamingLibriSpeechDataset(\n",
    "        subset=subset,\n",
    "        split=split,\n",
    "        sampling_rate=16_000,\n",
    "        streaming=True,\n",
    "        max_samples=max_samples,\n",
    "    )\n",
    "\n",
    "    char_total = 0\n",
    "    char_errors = 0\n",
    "    word_total = 0\n",
    "    word_errors = 0\n",
    "\n",
    "    for idx, (waveform, _logmel, sample_rate, transcript) in enumerate(dataset, start=1):\n",
    "        ref = tokenizer.normalize(transcript)\n",
    "        hyp = tokenizer.normalize(\n",
    "            transcribe_waveform(\n",
    "                waveform,\n",
    "                sample_rate,\n",
    "                model,\n",
    "                tokenizer,\n",
    "                logmel_extractor,\n",
    "                decode=decode,\n",
    "                beam_width=beam_width,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        char_errors += _levenshtein(ref, hyp)\n",
    "        char_total += len(ref)\n",
    "        word_errors += _levenshtein(ref.split(), hyp.split())\n",
    "        word_total += max(len(ref.split()), 1)\n",
    "\n",
    "    cer = char_errors / max(char_total, 1)\n",
    "    wer = word_errors / max(word_total, 1)\n",
    "    print(f\"Split={split}, samples={max_samples}, decode={decode}, beam_width={beam_width}\")\n",
    "    print(f\"CER={cer:.4f}, WER={wer:.4f}\")\n",
    "    return {\"cer\": cer, \"wer\": wer, \"samples\": max_samples}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea11703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split=validation, samples=20, decode=beam, beam_width=10\n",
      "CER=0.2716, WER=0.6386\n",
      "Split=test, samples=20, decode=beam, beam_width=10\n",
      "CER=0.2811, WER=0.6871\n"
     ]
    }
   ],
   "source": [
    "# Example: run after the first two cells have executed\n",
    "metrics_dev = evaluate_librispeech(split=\"validation\", max_samples=20, decode=\"beam\", beam_width=10)\n",
    "metrics_test = evaluate_librispeech(split=\"test\", max_samples=20, decode=\"beam\", beam_width=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c0b833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "her lile it eat gelar im love deter od oldite demolegeot enede e lorg a an de quint e lad loreded to da he won wet in toge mole and lined in uorig e be doal i e te ganis blel o tedit it anger wy beleded gher klolis te li no\n"
     ]
    }
   ],
   "source": [
    "# Transcribe a local WAV file using the loaded model\n",
    "wav_path = \"audio_tests/epi.wav\"  # change to your file\n",
    "transcript = transcribe_file(\n",
    "    wav_path,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    logmel_extractor,\n",
    "    decode=\"beam\",\n",
    "    beam_width=10,\n",
    ")\n",
    "print(transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e6986cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: it is you who are mistaken raoul i have read his distress in his eyes in his every gesture and action the whole day\n",
      "Decoded:   it is e wiv was daken erl i arve lid is distres in his es in his every gestur at paction the whole day\n"
     ]
    }
   ],
   "source": [
    "# Transcribe a random sample from the LibriSpeech test split\n",
    "import random\n",
    "from streaming_librispeech_dataset import StreamingLibriSpeechDataset\n",
    "\n",
    "max_samples = 50  # how many items to scan from the stream\n",
    "target_idx = random.randint(0, max_samples - 1)\n",
    "\n",
    "dataset = StreamingLibriSpeechDataset(\n",
    "    subset=\"clean\",\n",
    "    split=\"test\",\n",
    "    sampling_rate=16_000,\n",
    "    streaming=True,\n",
    "    max_samples=max_samples,\n",
    ")\n",
    "\n",
    "chosen = None\n",
    "for i, (waveform, _logmel, sample_rate, transcript) in enumerate(dataset):\n",
    "    if i == target_idx:\n",
    "        chosen = (waveform, sample_rate, transcript)\n",
    "        break\n",
    "\n",
    "if chosen is None:\n",
    "    raise RuntimeError(f\"Failed to grab sample {target_idx} from first {max_samples} items.\")\n",
    "\n",
    "waveform, sample_rate, reference = chosen\n",
    "decoded = transcribe_waveform(\n",
    "    waveform,\n",
    "    sample_rate,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    logmel_extractor,\n",
    "    decode=\"beam\",\n",
    "    beam_width=10,\n",
    ")\n",
    "print(f\"Reference: {tokenizer.normalize(reference)}\")\n",
    "print(f\"Decoded:   {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e296492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
